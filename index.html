<h1>Team 4 Project</h1>
<subtitle>Christopher Barré | Nidhi Malhotra | Amber Rothe</subtitle>
<hr>
<h2>Introduction/Problem Definition</h2>
<p>
  <h3>High Level Description and Motivation</h3>
  Minimally invasive surgery often requires working in areas that are not visible to the surgeon. 
  There are multiple techniques for helping the surgeon visualize his or her surgical tools in relation 
  to the anatomy. X-Ray flouroscopy can be used; however, this exposes healthcare professionals to radiation. 
  Magnetic Resonance Imaging (MRI) is another option, but this has several shortcomings. It is expensive. It restricts the movement of the 
  surgeon, because the patient must be inside the bore of the MRI machine. Finally, no ferromagnetic surgical tools may be 
  used near an MRI machine. Another option is ultrasound imaging, but it typically has low resolution and requires an expert 
  sonographer to operate. 
  <br><br>
  As cameras have gotten smaller, it has become possible to put a small camera called an endoscope through the surgical aperature. 
  This camera gives the physician a real time view of his or her surgical tools without the drawbacks of other imaging modalities. 
  Meanwhile, robotics has found its way into operating rooms. Commercially available platforms such as the DaVinci and Rosa robots 
  have already been used in surgery, and many new devices are under development. These robotic surgical tools give physicians greater
  dexterity and may even assist in path planning, compensate for the surgeon's unwanted hand motion, or allow surgeons to conduct surgeries
  remotely. 
  <br><br>
  As minimally invasive surgeries incorporate more and more robotics, endoscope cameras have become an attractive option for providing imaging feedback to the surgeon.
  Using this imaging feedback coupled with vision algorithms can help in surgical tool segmentation and tool tracking in the scene. 
  By obtaining the position and configuration of the surgical tool relative to the surrounding 
  tissue several surgical procedural parts can be semi-automated using closed-loop control. However, the task is not without challenges. The image quality of endoscope 
  cameras tends to be poor and distorted, and stereo images are usually not available. 
  <h3>Specific Problem Definition</h3>
  For this project, we will attempt two main tasks that act as first steps towards 3D global frame pose estimation, outlined as follows. 
  <h4>Segmentation</h4>
  The segmentation task involves determining which pixels in an endoscope camera still frame are 
  associated with the robotic surgical tool. The segmentation task is perhaps
  not particularly useful on its own, but may be helpful for pose estimation.
  <h4>Tracking/Pose Estimation</h4>
  The first step towards image based robotic control is pose tracking. For the scope of this project,
  we will track four properties of each tool in the frame.
  <ul>
    <li>Tracked point: A pixel location on the tool at the joint between the shaft and the head</li>
    <li>Shaft axis: A 2D vector representing the orientation of the tool shaft</li>
    <li>Head axis: A 2D vector representing the orientation of the tool head</li>
    <li>Clasper angle: An angle representing how wide the tool claspers are open</li>
  </ul>
  
  <img src="guide.png" style="width:50%">
  <br>
  Image source: EndoVis 15

</p>

<h2>Related Works</h2>
<!-- <p>
  Describe related works in your problem space (research papers, libraries/tools, etc) for existing solutions for this problem or adjacent areas. Make sure to cite papers you reference!
  ○	Note: Related work should have between 2 to 6 sentences for each work citing. Please cite works following IEEE guidelines. Organize related work into different subsections based on similarity of approaches.
</p> -->
<h3>General CV Techniques in Medical Imaging</h3>
<p>
    [1] Choudhary, A. (n.d.). Guidewire detection and 3D-Reconstruction for image-guided surgery. https://anic46.github.io/data/Guidewire_Project.pdf
</p>
<blockquote>
    This report was written by a previous masters student in our own lab as part of a special projects class several years ago. It provides a good overview of the
    challenges and strategies related to identifying pose of guidewires under X-ray. To identify the guidewire, the author first applied distortion, isocenter correction,
    histogram equalization, and smoothing to correct the X-ray images. He then applied a large median filter to remove the thin guidewire from the image, while also
    keeping an unfiltered copy. Next, he applied DoG and B-COSFIRE based filters to detect vessels in both image. He thresholded the output to obtain one mask from each image,
    then subtracted the masks from each other to obtain a mask with only the guidewire. Finally, he used curvelinear structure detection to highlight ridge structures to refine
    the segmentation of the guidewire. While developing this procedure, the author also tried some other approaches, which were found to be suboptimal. He tried using Meijering
    neuriteness, Sato tubeness, Frangi vesselness and Hessian vesselness filters to identify the vessels. He also attempted to use SIFT to identify guidewire features and track
    it across multiple images.
</blockquote>
<h3>Segmentation of Tools from Endoscope Images</h3>
<p>
    [2] C. Tomasini, I. Alonso, L. Riazuelo, and A. C. Murillo, “Efficient tool segmentation for endoscopic videos in the wild,” in Proceedings of The 5th
    International Conference on Medical Imaging with Deep Learning, E. Konukoglu, B. Menze, A. Venkataraman, C. Baumgartner, Q. Dou, and S. Albarqouni, Eds.,
    in Proceedings of Machine Learning Research, vol. 172. PMLR, Feb. 2022, pp. 1218–1234. [Online]. Available: https://proceedings.mlr.press/v172/tomasini22a.html
</p>
<blockquote>
    This paper discusses segmenting tools in endoscope views, which is the first step for pose estimation. The paper mentions several techniques for segmenting endoscope tools, including
    UNet and related models, MF-TAPNet, DMNet, MiniNet, and Binary frame classification.
</blockquote>

<h3>Tracking/Pose Estimation of Tools from Endoscope Images</h3>
<p>
    [3] M. Yoshimura, M. M. Marinho, K. Harada and M. Mitsuishi, "Single-Shot Pose Estimation of Surgical Robot Instruments’ Shafts from Monocular Endoscopic Images," 2020 IEEE International Conference on
    Robotics and Automation (ICRA), Paris, France, 2020, pp. 9960-9966, doi: 10.1109/ICRA40945.2020.9196779
</p>
<blockquote>
    This paper was able to estimate pose from a single monocular endoscope image using a modified SSD-6D
    network.
</blockquote>
<p>
    [4] Cabras P, Nageotte F, Zanne P, Doignon C. “An adaptive and fully automatic method for estimating
    the 3D position of bendable instruments using endoscopic images.” Int J Med Robot. 2017 Dec;13(4).
    doi: 10.1002/rcs.1812. Epub 2017 Apr 7. PMID: 28387448.
</p>
<blockquote>
    This paper also used a monocular camera. The authors leveraged colored markers
    attatched to the corners of the instrument. In this paper, the instrument is segmented
    using a graph-based method. The corners of markers are
    extracted by detecting the color change. The features are used to estimate the
    3D pose of the instrument using an adaptive model.
</blockquote>
<p>
    [5] Chiu, Zih-Yun et al. “Real-Time Constrained 6D Object-Pose Tracking of An In-Hand
    Suture Needle for Minimally Invasive Robotic Surgery.” 2023 IEEE International Conference
    on Robotics and Automation (ICRA) (2022): 4761-4767.
</p>
<blockquote>
    In this paper the authors tract the 6D pose of an in-hand suture needle. The main contibution
    of the work is including contraints such that the needle is in a feasible grasping
    manifold of the gripper. The authors use a reparameterization by defining the pose of the
    needle with respect to the end effector frame. This makes it easy to define the grasping constraints.
    A marker-less deep neural network is used for feature detection.
</blockquote>
<p>
    [6] R. Hao, O. Özgüner and M. C. Çavuşoğlu, "Vision-Based Surgical Tool Pose Estimation for the da Vinci® Robotic Surgical System,"
    2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, Spain, 2018, pp. 1298-1305, doi: 10.1109/IROS.2018.8594471.
</p>
<blockquote>
    The authors are able to identify the location of the tool visually in the camera frame and compare this with the expected view of the tool obtained from robot kinematics.
    They are then able to use the difference in location to update their particle filter to perform Bayesian estimation. This paper uses stero cameras rather than a single view,
    but the principles of what they are doing should be applicable to a single view as well, but our results might not be as good. Overall, this paper represents
    the "final step" and a stretch goal for our project: actually using the information obtained from images
    to control robot motion.
</blockquote>
<p>
    [7] Mobarakol Islam, Vibashan VS, Chwee Ming Lim, Hongliang Ren, ST-MTL: Spatio-Temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery,
    Medical Image Analysis, Volume 67, 2021.
</p>
<blockquote>
    The authors in this work propose an end-to-end trainable Spatio-Temporal Multi-Task Learning (ST-MTL) model for real-time surgical
    instrument segmentation. A novel Asynchronous Spatio-Temporal Optimization (ASTO) to optimize the ST-MTL model is implemented
    for shared-encoder and task-aware decoders.
</blockquote>

<h3>Our Project in Context</h3>
We are all new to computer vision, so we do not expect to develop novel or state of the art approaches for
this task. However, unlike previous papers which tend to focus on a single method, we will investigate a
variety of methods for tool segmentation and tracking and provide side by side results.

<h2>Methods/Approach</h2>

<h3>Segmentation</h3>
  For the segmentation task, we tried several different approaches, outlined below. 

  <h4>Color Based Segmentation</h4>
  <p>
    As a first try, this very simple and naiive algorithm was used to segment the 
    images. Most tissues inside the body are red in color, leading to our hypothesis
    that selecting all the non-red pixels in an image might be a good first attempt at
    segmentation.
    <br>
    <img src="amber_segmentation_figure.png" style="width:50%">
    <br>
    First, a gaussian blur was applied to the image to remove high frequency noise. 
    Next, the frame was converted into the HSV color space. A mask was created such that 
    all the non-red pixels were true. Next, morphological operations erosion and dilation were 
    preformed to remove small speckles in the mask and join together larger non-red regions. 
    Then, the area with the largest number of connected "true" pixels was found, and all other
    "true" areas were discarded. 
    <br>
    <img src="amber_segmentation_procedure.png" style="width:80%">

    
  </p>

<h4>Blob Detection Based Segmentation Method</h4>
<p>
    We also thought that a blob detection algorithm for identifying circle-like regions could
    be used to segment the different pieces of the tooltip.  A difference of Gaussians filter
    was used to approximate the Laplacian of Gaussian and was applied to the RGB image, after
    which the results were averaged together into grayscale.  With a kernel size similar to the
    size of the tooltip in the frame, identifying regions with the maximum response for the
    difference of Gaussians filter should point to the regions containing the tooltip.  This
    kernel size can be identified by a least squares optimization that minimizes the error for
    the segmentation.
</p>

<h4>SIFT based segmentation</h4>
<p>
    In this approach, we try to find keypoints in the scene for tool recognition. On the first attempt by finding keypoints using in-built sift function,
    a large number of keypoints including - the tool tips, nerve regions and bright regions are recognized as key features. In this approach the tool tip is
    segmented in most cases but in the tool shaft region which is black no keypoints are detected.
    Several pre-processing steps are attempted but all of them have drawbacks - initially bluring the image is attempted which leads to finding a relatively
    smaller set of keypoints and give a blurred edge of the tool . By sharpening the image more keypoints are found
    which give a good edge for the tool but several bright regions which are not the tool in the image are also detected.

    We attempt on using some information from detecting the keypoints for obtaining a good tip detection of the tool while tracking the pose. For segmentation though,
    this approach needs to be combined with another approach to get good results.
</p>

<h4>Learning based segmentation</h4>
<p>
  We attempt segmenting the left surgical instrument from the video frame. One video frame is used and split for the training and validation data. Post training, a different video is used to test the performance of the trained model.
  We use a UNET model, following the approach in <a href="https://github.com/zhixuhao/unet">[1]</a>,<a href="https://github.com/PrathamLearnsToCode/Semantic_Segmentation_Surgery-Tools_U-Net">[2]</a>, for our dataset. The baseline model is trained on our dataset and we evaluate the performance of the model. The architecture of the UNET is shown below.
  <br>
  <img src="unet_arch.png" style="width:50%">
  <br>
  Description of architecture: The UNET model is commonly used for biomedical datasets. It has an encoder-decoder architecture. The “encoder” extracts the features and captures the context of the scene. The “decoder” generates a segmentation map for accurate localization of the features. This is achieved by the encoder reducing the spatial dimensions of the input image and increasing the number of feature channels, while the decoder would increase the spatial dimensions of the feature maps. 
  In the model the “jaccard distance” which gives the intersection over union is used as the loss function to train the model. 
</p>

<h3>Tracking</h3>

<h4>Color Based Tracking Method</h4>
<p>
    An attempt was made to recover six of the seven tracking values using only traditional CV methods (without learning.) The following algorithms were used to recover each item.
    <ul>
        <li>
            tracked point: The location of the "tracked point" is the point where the dark colored shaft and the light colored
            head meet. The image was first segmented to recover the whole tool (see above). The head was then segmented out from the tool. Since the head and shaft are similar colors,
            segmentation by color could not be used. Instead, it was noted that the tool head has many more edges than the shaft. Canny edge detection was used to highlight all the edges
            on the tool. Morphological dilation and erosion of the edges was then performed to create an approximation of the shape of the head.
            The head mask was subtracted from the whole tool to obtain the shaft alone.
            Then, morphological dilation was used on the shaft mask. Next, a logical AND was applied to find the overlap between the shaft and the head. Finally, the
            centroid of this overlap region was found. The centroid became our approximation for the tracked point.
        </li>
        <li>
            shaft axis:  These values represent a cartesian unit vector indicating the orientation of the tool shaft. The image was first segmented to recover
            the whole tool. Root mean square was used to find the line that best fit the mask. The vector representing the slope of the best fit line was computed.
            In the results section below, the angle difference between the ground truth and computed vector are reported.
        </li>
        <li>
            head_axis:  These values represent a cartesian unit vector indicating the orientation of the tool head. The image was first segmented to recover
            the the head alone. Root mean square was used to find the line that best fit the head mask. The vector representing the slope of the best fit line was computed.
            In the results section below, the angle difference between the ground truth and computed vector are reported.
        </li>
        <li>clasper_angle: Reconstructing this value was not attempted using this method.</li>
    </ul>
</p>

<h4>Machine Learning Based Tracking</h4>
<img src="tracking_network_diagram.png" style="width:50%">
<p>
    We also attempted a learning approach for estimating the tool poses in the frame of the image.  We used a convolutional neural network that inputs the three
    channel image and outputs the 7 pose values for each tool.  Our network consists of 4 blocks, where each block contains a convolution followed by a 3x3 max pool and a ReLU
    nonlinear activation.  We used kernels of size 3x10x10, 7x7, 5x3, and 5x5, in that order.  The first convolution was a three dimensional convolution of depth 3 in order to
    bring the 3-channel image into a 1-channel format without just throwing away the color data by converting to grayscale first, considering the success of segmentation using
    color so far.  After the convolution, we flattened the data and passed through three fully connected linear layers to bring it down to the 1x14 size that we desired.
    This 1x14 array can be split into the two tool poses which are length 7 each.  We also included dropouts of p=0.1 after the colvolutional blocks and of p=0.5 in between the
    fully connected layers.  The model was trained using an Adam optimizer at a learning rate of 1e-3 with an 8e-4 weight decay with a batch size of 30 and 30 epochs.
    For the loss function, we used a mean squared error loss.  Additionally, to prevent the position tracking from dominating the gradients in the training process, we normalized
    the position data for the height and width of the image, such that the model predicts a value from 0.0 to 1.0 for the positions, which give the proper pixel values when multiplied
    again by the height and width of the image.
</p>



<h3>Contribution</h3>
  While none of our approaches are novel, we contribute by providing a side by side comparison to show which are
  applicable in this problem domain and which are not. 

<h3>Intuition</h3>
  By experimenting with many different methods, we can find at least one
  method that is suitable for our task. We predict that the more classical approaches will work well
  for the data we have, but might be harder to generalize to other lighting conditions and styles of tools. 
  The learning approaches are more likely to be generalizable. 


<h2>Experiments</h2>

<p>
  We used the <a href="https://endovissub-instrument.grand-challenge.org/Data/">EndoVis 15 dataset,</a> which provides endoscope views of robotics tools. 
  It also provides ground truth data for both tracking and segmentation tasks. There are 6 videos in the 
  test dataset. Each frame is 720x576 pixels and there is a total of 2:45 minutes of video to test on. 
  The dataset is pre-divided into test and train segments. For both test and train, ground truth segmenation
  and instrument pose are provided. 

</p>

<h3>Segmentation</h3>

<h4>Input</h4>
The input was the endoscopic camera video from the EndoVis 15 dataset described above. 
<h4>Desired Output</h4>
The desired output is a binary image the same size as the original image. True pixels correspond
to the tool in the image. 
<hv>Success metric</hv>
Intersection Over Union and the DICE coefficient (double intersection over total area) were used to measure success, comparing our segmentation to the ground-truth 
segmentation provided by EndoVis. 

<h4>Strategies</h4>
<h5>Color Based Segmentation</h5>
  <p>
    We gave the test data from the EndoVis dataset to the color based segmentation algorithm and recorded
    the Intersection Over Union metric for each frame of the test data video. 
    This algorithm worked suprisingly ok considering how basic it is.
    However, it does have some
    shortcomings. The threshold for what is considered "red" was chosen manually and may not be
    generalizable to other data sets where the lighting is different. In the future, perhaps these
    parameters could be chosen using learning.
    <br>
    Overall, we acheived an average IoU of 53.8% with this algorithm.
</p>
<img src="color_segmentation_example.png" style="width:50%">
<br>
<img src="color_segmentation_iou.png" style="width:50%">

  <h5>Blob Detection Based Segmentation Method</h5>
  <p>
      The use of the blob detection algorithm for identifying circle-like features in the image frames
      did not work very well at all. Because of the the large blob size, it was necessary to use a
      difference of Gaussians instead of the preferred Laplacian of Guassian.  For a broad range of kernel
      sizes, the peak regions from this filter are spread haphazardly amongst the tooltip, shaft, and tissue, as shown below.
      Even with using learning to identify an optimal kernel size, the usefulness of this method seems unlikely.
  </p>
  <img src="blob_failure_evidence.png" style="width:50%">

  <h5>SIFT for segmentation</h5>
  <p>
    This algorithm works well for detecting the tool tip region in some cases but fails to detect the shaft of the tool. Need to combine with another approach for better results for segmentation. For tracking the features are matched well in different frames. 
    We can use optical flow later to detect the features with move withing frames for tracking.
    <br>
    Example sift segmentation results
    <br>
    <img src="sift_output.png" style="width:50%">
    <img src="sift_output2.png" style="width:50%">
    <br>
    Keypoint matches - can be further integrated with optical flow for tracking
    <br>
    <img src="keypoints_matched.png" style="width:50%">
  </p>

  <h5>Machine Learning Based Segmentation</h5>
  <p>
    <li>Different optimizers – “adam”, “sgd”, “rmsprop”, “adamax” are tried to compare the accuracy. </li>
    <img src="iou_opt.png" style="width:50%">
    <li>We obtain a 99.5% train accuracy, 78% test accuracy for the best trained model. The loss and accuracy during training is shown for the best trained model.</li>
    <img src="model_loss_and_accuracy.png" style="width:50%">
    <li>The input, mask and segmented image for a test case is also shown below.</li>
    <img src="unet_final_output.png" style="width:50%">
  </p>

<h3>Tracking</h3>

<h4>Input</h4>
The input was the endoscopic camera video from the EndoVis 15 dataset described above. 
<h4>Desired Output</h4>
The desired output is a 1x7 array with the following values:
<ul>
  <li>center_point _x & center_point_y: Pixel coordinates of the center point for
    instrument, the center point is defined as the intersection between the instrument axis
    and the border between metal and plastic on the shaft.</li>
  <li>shaft_axis_x & shaft_axis_y: Normalized axis vector of instrument</li>
  <li>head_axis_x & head_axis_y: Normalized axis vector of instrument head</li>
  <li>clasper_angle: The angle in degrees between the claspers.</li>
</ul>

to the tool in the image. 
<hv>Success metric</hv>
We used the error between the actual and expected pose values.  We also took particular notice of the Euclidean distance between the true and estimated tracked point
because that is the evaluation criterion used in the EndoVis competition.

<h4>Strategies</h4>
  <h5>Classical Tracking Method</h5>
  <p>
    The average errors, in pixels, were:
    <ul>
        <li>tracked point x RMS Error: 217.443041 pixels (30.20% of image width)</li>
        <li>tracked point y RMS Error: 107.676291 pixels (18.69% of image height)</li>
        <li>shaft angle RMS Error: 72.999660 degrees</li>
        <li>head angle RMS Error: 50.790612 degrees</li>
    </ul>
    <br>
    For comparison, the total size of the images were 720x576 pixels.
    <br>
    However, the averages could be pulled high by the frames when the model really was confused, even though it was accurate most of the time.
    So we additionally present the median absolute error:

    <ul>
        <li>tracked point x Median Error: 36.381992 pixels (5.05% of image width)</li>
        <li>tracked point y Median Error: 40.129398 pixels (6.97% of image height)</li>
        <li>shaft angle Median Error: 4.996736 degrees</li>
        <li>head angle Median Error: 14.338042 degrees</li>
    </ul>

    <br>
    <img src="color_tracking_histograms.png" style="width:50%">
    <br>
    Finding the grasper angle was not attempted using this method.
    <br>
    Overall, this method was somewhat hit or miss. The median error was relatively small - less than 10% of the image height.
    However, the mean error was much larger, indicating that sometimes when the model was wrong, it was really wrong.
    <br>
    In the following image,
    <ul>
        <li>Magenta: Guess for the left tool tracked point</li>
        <li>Cyan: Ground truth for the left tool tracked point</li>
        <li>Yellow: Guess for the right tool tracked point</li>
        <li>White: Ground truth for the right tool tracked point</li>
        <li>Blue line: Head axis</li>
        <li>Red line: Shaft axis</li>
    </ul>


    <img src="classical_tracking_screenshot.PNG" style="width:50%">

</p>

<h4>Machine Learning Tracking Approach</h4>
<p>
    The average errors, in pixels, were:
    <ul>
        <li>tracked point x RMS Error: 95.718955 pixels (13.29% of image width)</li>
        <li>tracked point y RMS Error: 81.962481 pixels (14.23% of image height)</li>
        <li>tracked point Euclidean distance RMS Error: 126.015739 pixels</li>
        <li>shaft angle RMS Error: 39.735864 degrees</li>
        <li>head angle RMS Error: 19.556128 degrees</li>
        <li>clasper angle RMS Error: 11.725729 degrees</li>
    </ul>
    <br>
    For comparison, the total size of the images were 720x576 pixels.
    <br>
    However, the model is optimized around the mean squared error, so using this as the metric for evaluation is somewhat biased.
    So we additionally present the median absolute error:

    <ul>
        <li>tracked point x Median Error: 55.302938 pixels (7.68% of image width)</li>
        <li>tracked point y Median Error: 54.814855 pixels (9.52% of image height)</li>
        <li>tracked point Euclidean distance Median Error: 90.220586 pixels</li>
        <li>shaft angle Median Error: 7.258671 degrees</li>
        <li>head angle Median Error: 9.517581 degrees</li>
        <li>clasper angle Median Error: 5.508275 degrees</li>
    </ul>
    <br>
    <img src="learning_based_tracking_plots.png" style="width:50%">
    <br>
    Overall, this method was not very effective, as shown when watching the videos of the estimated pose in frame, where it is clear that the model gets pigeon holed
    into always guessing close to a predicted average position for each video, and barely deviates from that position for any given frame.  This revealed that our
    neural network was most likely not deep enough to properly address the task.  However, unlike with segmentation, we were unaware of any ready-made networks
    that we could use to solve this problem, so this method was left as is.
</p>

  

<h2>Results</h2>

<h3>Baseline</h3>
<!--■	How do prior works perform on this task? It’s best to have a quantitative comparison 
  using your metric for success. If that is not possible, a qualitative result will suffice. 
■	You are required to have at least one comparison-->
We are using the results from the EndoVis Grand Challenge as a baseline for our experiments.  For tracking, the smallest tip position error was 34.9 pixels.
For segmentation, the best DICE coefficient was 84%.

<h3>Key Result</h3>
The key results of our experiments are summarized below
<br>
    <img src="results_summary.png" style="width:50%">
  <br>
<!--■	Clearly present your key result. This should include both your performance according to your 
  metric of success defined above and a qualitative output example from your system. -->
<!--■	Specify what variants of your approach you tried to make progress towards finding a solution. 
■	Ultimately, describe your final results. Did your approach work? If not, explain why you 
believe it did not. 

-->

<h2>Discussion</h2>
<!--○	Provide a reflective paragraph about your project. Summarize what you accomplished, 
  what you learned, and what remains for future works. -->
  With this project, a comparison of different segmentation and tracking algorithms was accomplished.
  Over the course of this project, we learned how to implement a variety of computer vision
  algorithms and strategies. We were able to employ concepts we learned about in class to 
  solve a real-world problem. We used concepts including color spaces, filtering, thresholding,
  blob detection, SIFT, and learning. We also became more comfortable with computer vision
  and learning tools including OpenCV and Pytorch. In the future, we could try the strategies
  from this project with other endoscopes and tools.
<br>
    Another potential avenue for future work would be further exploration in the variations of
    the methods we employed, as well as a combination of those methods.  Particularly, applying
    the classical tracking methdods used to a video segmented via the learning methods used
    has the potential to perform better than either on their own.

<h2>Challenges Encountered</h2>
<!--○	What challenges did your team encounter throughout the project?
○	If you were to start over today, is that a different strategy you would take 
to make progress on your overall problem statement?-->
We initially spent a lot of time working on classical techniques for doing segmentation and
tracking, with mediocre results. A learning approach is probably better at capturing the 
nuances of the image and is more reliable overall. If we were to do this project again, we
might have started with learning right away and saved more time to fine tune our 
model architecture.


<!-- <h2>What’s next</h2>
<p>What is your plan until the final project due date? What methods and experiments do you plan on running?
○	Note: Include a task list (can use a table) indicating each step you are planning and anticipated completion date.
  The next we would try combining classical approaches and learning based medthods.
<p>
    <li>Combine color segmentation with key features for tracking tool tip -  March 27 - April 2</li>

    <li>
        For segmentation
        <ul>
            <li> Setup pytorch environment - March 27 - April 2</li>
            <li> Try CNN for tool segmentation - April 3 - April 10 </li>
            <li> Use keypoints for as inputs to training model - April 3 - April 10 </li>
            <li> Try endoder-decoder architectures - April 11 - April 18 </li>
        </ul>
    </li>
    <li>For tracking
    <li> Try to implement optical flow for SIFT feature points to detect motion and track the pose of the instrument - April 1 - April 7
    <li>
        Try to implement LSTM for spatio temporal patterns of the instrument - April 11 - April 24
        <ul>
            <li> Use convolutional neural networks to extract visual features. </li>
            <li> Use color histograms, or sift keypoints as features. </li>
            <li> Can clip video to shorter frame length and randomly choose input from batch to reduce data size. </li>
            <li> Output layer should give pose. </li>
            <li> Use regularization to prevent overfitting. </li>
        </ul>
    </li>
</p> -->

<h2>Team Member Contributions</h2>
<!-- <p>
  Indicate what you anticipate each team member will contribute by the final project submission.
○	Note: List every member name and their corresponding tasks in bullet points – or you may simply assign team member names to the task list you created above.
</p> -->
<ul>
    <li>
        Christopher Barré
        <ul>
            <li>Experiment with blob-detection based segmentation</li>
            <li>Experiment with using learning for tracking</li>
        </ul>
    </li>
    <li>
        Nidhi Malhotra
        <ul>
            <li>Experiment with SIFT for segmentation</li>
            <li> Experiment with learning for segmentation</li>
         </ul>
    </li>
    <li>
        Amber Rothe
        <ul>
            <li>Set up repository</li>
            <li>Make website template</li>
            <li>Write utility code for loading data</li>
            <li>Write the main architecture of the program for classical approaches</li>
            <li>Write utility code for visualizing results</li>
            <li>Experiment with color-based segmentation</li>
            <li>Experiment with the classical tracking approach</li>
        </ul>
    </li>
</ul>
